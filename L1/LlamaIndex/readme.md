# ä¹¦ç”Ÿå¤§æ¨¡å‹å®æˆ˜ LlamaIndex RAG

æ‰€æœ‰å®éªŒåŸºäº InternStudio å¹³å°è¿›è¡Œã€‚

## ç¯å¢ƒé…ç½®

```bash
# åˆ›å»ºç¯å¢ƒ
conda create -n llamaindex python=3.10 -y
conda activate llamaindex
# å®‰è£…ä¾èµ–åŒ…
pip install llama-index==0.11.20
pip install llama-index-llms-replicate==0.3.0
pip install llama-index-llms-openai-like==0.2.0
pip install llama-index-embeddings-huggingface==0.3.1
pip install llama-index-embeddings-instructor==0.2.1
# InternStudio cuda12 ç¯å¢ƒè¿è¡ŒæŠ¥é”™ï¼Œè¿™é‡Œé‡‡ç”¨ cuda11.7 ç¯å¢ƒ
# pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu121
pip3 install torch==2.5.1 torchvision==0.20.1 --index-url https://download.pytorch.org/whl/cu118
```


### ä¸‹è½½æ¨¡å‹å’Œæ•°æ®

```bash
cd ~
mkdir llamaindex_demo model

# ä¸‹è½½æ¨¡å‹
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 --local-dir /root/model/sentence-transformer

# ä¸‹è½½ NLTK èµ„æº
git clone https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages
cd nltk_data
mv packages/* ./
unzip punkt.zip
cd ../taggers
unzip averaged_perceptron_tagger.zip
```

## åŸºç¡€ä»»åŠ¡


>- ä»»åŠ¡è¦æ±‚1ï¼ˆå¿…åšï¼Œå‚è€ƒreadme_api.mdï¼‰ï¼šåŸºäº LlamaIndex æ„å»ºè‡ªå·±çš„ RAG çŸ¥è¯†åº“ï¼Œå¯»æ‰¾ä¸€ä¸ª>é—®é¢˜ A åœ¨ä½¿ç”¨ LlamaIndex ä¹‹å‰ æµ¦è¯­ API ä¸ä¼šå›ç­”ï¼Œå€ŸåŠ© LlamaIndex å æµ¦è¯­ API å…·å¤‡å›>ç­” A çš„èƒ½åŠ›ï¼Œæˆªå›¾ä¿å­˜ã€‚æ³¨æ„ï¼šå†™åšå®¢æäº¤ä½œä¸šæ—¶åˆ‡è®°ä¸è¦æ³„æ¼è‡ªå·± api_keyï¼
>
>- ä»»åŠ¡è¦æ±‚2ï¼ˆå¯é€‰ï¼Œå‚è€ƒreadme.mdï¼‰ï¼šåŸºäº LlamaIndex æ„å»ºè‡ªå·±çš„ RAG çŸ¥è¯†åº“ï¼Œå¯»æ‰¾ä¸€ä¸ªé—®é¢˜ >A åœ¨ä½¿ç”¨ LlamaIndex ä¹‹å‰ InternLM2-Chat-1.8B æ¨¡å‹ä¸ä¼šå›ç­”ï¼Œå€ŸåŠ© LlamaIndex å >InternLM2-Chat-1.8B æ¨¡å‹å…·å¤‡å›ç­” A çš„èƒ½åŠ›ï¼Œæˆªå›¾ä¿å­˜ã€‚
>
>- ä»»åŠ¡è¦æ±‚3ï¼ˆä¼˜ç§€å­¦å‘˜å¿…åšï¼‰ ï¼šå°† Streamlit+LlamaIndex+æµ¦è¯­APIçš„ Space éƒ¨ç½²åˆ° Hugging >Faceã€‚


### åŸºäº API çš„ RAG

æµ¦è¯­å®˜ç½‘å’Œç¡…åŸºæµåŠ¨éƒ½æä¾›äº†InternLMçš„ç±»OpenAIæ¥å£æ ¼å¼çš„å…è´¹çš„ APIï¼Œå¯ä»¥è®¿é—®ä»¥ä¸‹ä¸¤ä¸ªäº†è§£ä¸¤ä¸ª API çš„ä½¿ç”¨æ–¹æ³•å’Œ Keyã€‚

- æµ¦è¯­å®˜æ–¹ APIï¼šhttps://internlm.intern-ai.org.cn/api/document
- ç¡…åŸºæµåŠ¨ï¼šhttps://cloud.siliconflow.cn/models?mfs=internlm

é¦–å…ˆæˆ‘ä»¬æµ‹è¯•ä¸ç”¨ RAG ï¼Œç›´æ¥è¯¢é—® LLM ï¼š

```python
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# offical internlm
base_url = "https://internlm-chat.intern-ai.org.cn/puyu/api/v1/"
api_key = os.getenv('PUYU_API_KEY')
model = 'internlm2.5-latest'

# # siliconflow
# base_url = "https://api.siliconflow.cn/v1"
# api_key = os.getenv('SILICON_API_KEY')
# model = 'internlm/internlm2_5-7b-chat'

client = OpenAI(api_key=api_key, base_url=base_url)

chat_rsp = client.chat.completions.create(
    model=model,
    messages=[
        {'role': 'user', 'content': 'ç¡…åŸºæµåŠ¨å…¬å¸ä»‹ç»'}
    ]
)

for choice in chat_rsp.choices:
    print(choice.message.content)
```

![](./llamaindex_01.jpg)


InternLM2.5 ä¼šç›´æ¥å›å¤ä¸çŸ¥é“ã€‚


æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ©ç”¨ LlamaxIndex æ„å»ºä¸€ä¸ªç®€å•çš„ RAG æ£€ç´¢ã€‚

1. æ‰¾ä¸€æ®µ ç¡…åŸºæµåŠ¨ å…¬å¸çš„ç®€ä»‹æ–‡å­—ï¼Œå­˜ä¸º `/root/llamaindex_demo/data1/silicon_intro.md` æ–‡ä»¶ã€‚
2. å¡«å…¥ä»¥ä¸‹ä»£ç ï¼Œè¿è¡Œ


```python
import os
os.environ['NLTK_DATA'] = '/root/nltk/data'

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.settings import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.legacy.callbacks import CallbackManager
from llama_index.llms.openai_like import OpenAILike

from dotenv import load_dotenv

load_dotenv()

# create an instance of CallbackManager
callback_manager = CallbackManager()

api_base_url =  "https://internlm-chat.intern-ai.org.cn/puyu/api/v1/"
model = "internlm2.5-latest"
api_key = os.environ['PUYU_API_KEY']

# # siliconflow
# base_url = "https://api.siliconflow.cn/v1"
# api_key = os.getenv('SILICON_API_KEY')
# model = 'internlm/internlm2_5-7b-chat'

llm = OpenAILike(
    model=model,
    api_base=api_base_url,
    api_key=api_key,
    is_chat_model=True,
    callback_manager=callback_manager
)

#
embed_model = HuggingFaceEmbedding(
    model_name='/root/model/sentence-transformer'
)
# å°†åˆ›å»ºçš„åµŒå…¥æ¨¡å‹èµ‹å€¼ç»™å…¨å±€è®¾ç½®çš„ embed_model å±æ€§ï¼Œ
# è¿™æ ·åœ¨åç»­çš„ç´¢å¼•æ„å»ºè¿‡ç¨‹ä¸­å°±ä¼šä½¿ç”¨è¿™ä¸ªæ¨¡å‹ã€‚
Settings.embed_model = embed_model

# åˆå§‹åŒ– llm
Settings.llm = llm

# ä»æŒ‡å®šç›®å½•è¯»å–æ‰€æœ‰æ–‡æ¡£ï¼Œå¹¶åŠ è½½æ•°æ®åˆ°å†…å­˜ä¸­
documents = SimpleDirectoryReader('/root/llamaindex_demo/data1').load_data()
# åˆ›å»ºä¸€ä¸ª VectorStoreIndex ï¼Œå¹¶ä½¿ç”¨ä¹‹å‰åŠ è½½çš„æ–‡æ¡£æ¥æ„å»ºç´¢å¼•
# æ­¤ç´¢å¼•å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶å­˜å‚¨è¿™äº›å‘é‡ä»¥ä¾¿äºå¿«é€Ÿæ£€ç´¢
index = VectorStoreIndex.from_documents(documents)
# åˆ›å»ºä¸€ä¸ªæŸ¥è¯¢å¼•æ“ï¼Œæ¥æ”¶æŸ¥è¯¢å¹¶è¿”å›ç›¸å…³æ–‡æ¡£çš„å“åº”
query_engine = index.as_query_engine()
response = query_engine.query('ç»™æˆ‘ä»‹ç»ä¸€ä¸‹ç¡…åŸºæµåŠ¨ï¼ˆSiliconFlowï¼‰å…¬å¸')

print(response)
```

![](./llamaindex_02.jpg)

å¯ä»¥çœ‹åˆ°ï¼ŒåŠ å…¥å¤–æŒ‚çŸ¥è¯†åº“åï¼ŒLLM å°±å¯ä»¥æ¯”è¾ƒå‡†ç¡®çš„å›ç­”å‡ºä¹‹å‰ä¸çŸ¥é“çš„é—®é¢˜äº†ã€‚

### æ„å»º ModelScope Space åº”ç”¨

1. ç™»å½• [ModelScope](https://www.modelscope.cn/) åˆ›å»º åˆ›ç©ºé—´ RAG_Demo
2. å…‹éš†ä»“åº“

```bash
git lfs install
git clone http://oauth2:GIT_TOKEN@www.modelscope.cn/studios/pzhang199/RAG_Demo.git
```

3. æ„å»ºæ£€ç´¢æ•°æ® `data/siliconflow_intro.md`

4. åˆ›å»º `app.py` æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```python
import streamlit as st
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.legacy.callbacks import CallbackManager
from llama_index.llms.openai_like import OpenAILike

from modelscope.hub.snapshot_download import snapshot_download

embed_model_name_or_path = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'

cache_dir = './'
snapshot_download(embed_model_name_or_path, cache_dir=cache_dir)

# Create an instance of CallbackManager
callback_manager = CallbackManager()

api_base_url = "https://internlm-chat.intern-ai.org.cn/puyu/api/v1/"
model = "internlm2.5-latest"
api_key = "YOUR_API_KEY"

# api_base_url =  "https://api.siliconflow.cn/v1"
# model = "internlm/internlm2_5-7b-chat"
# api_key = "è¯·å¡«å†™ API Key"

llm = OpenAILike(model=model,
                 api_base=api_base_url,
                 api_key=api_key,
                 is_chat_model=True,
                 callback_manager=callback_manager)

st.set_page_config(page_title="llama_index_demo", page_icon="ğŸ¦œğŸ”—")
st.title("llama_index_demo")


# åˆå§‹åŒ–æ¨¡å‹
@st.cache_resource
def init_models():
    embed_model = HuggingFaceEmbedding(model_name=embed_model_name_or_path)
    Settings.embed_model = embed_model

    #ç”¨åˆå§‹åŒ–llm
    Settings.llm = llm

    documents = SimpleDirectoryReader("data").load_data()
    index = VectorStoreIndex.from_documents(documents)
    query_engine = index.as_query_engine()

    return query_engine


# æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
if 'query_engine' not in st.session_state:
    st.session_state['query_engine'] = init_models()


def greet2(question):
    response = st.session_state['query_engine'].query(question)
    return response


# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{
        "role": "assistant",
        "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
    }]

    # Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = [{
        "role": "assistant",
        "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"
    }]


st.sidebar.button('Clear Chat History', on_click=clear_chat_history)


# Function for generating LLaMA2 response
def generate_llama_index_response(prompt_input):
    return greet2(prompt_input)


# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Gegenerate_llama_index_response last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_llama_index_response(prompt)
            placeholder = st.empty()
            placeholder.markdown(response)
    message = {"role": "assistant", "content": response}
    st.session_state.messages.append(message)
```

4. ä¸Šä¼ è‡³ç©ºé—´

```bash
git add .
git commit -m "first version"
git push
```

5. ä¸Šçº¿ 

è®¿é—® [RAG_Demo](https://www.modelscope.cn/studios/pzhang199/RAG_Demo/) åˆ›ç©ºé—´ï¼Œè®¾ç½®ä¸ºå…¬å¼€ï¼Œç‚¹å‡»ä¸Šçº¿

![](./llamaindex_03.jpg)


